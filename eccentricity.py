# -*- coding: utf-8 -*-
"""eccentricity.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1swLKGJy5YETgAgNk-zK9YGT_FI5OoTkS
"""

import pandas as pd
import matplotlib.pyplot as plt
from pandas.plotting import scatter_matrix

data = pd.read_csv("nasa.csv")

parameters = [
    'pl_orbper', 'pl_orbsmax', 'pl_rade',
    'pl_bmasse', 'pl_orbeccen', 'pl_insol',
    'pl_orbincl', 'pl_ratdor', 'st_teff',
    'st_rad', 'st_mass', 'st_met', 'st_lum',
    'st_dens'
]

max_pl_orbsmax = data['pl_orbsmax'].max()
threshold_pl_orbsmax = max_pl_orbsmax * 0.75

parameter_data = data[parameters]

filtered_data = parameter_data[parameter_data <= threshold_pl_orbsmax]

scatter_matrix(filtered_data, figsize=(15, 15), alpha=0.5, diagonal='kde')
plt.show()

import pandas as pd
import numpy as np

file_path = "./nasa.csv"

data = pd.read_csv(file_path, comment='#')

features = [
       'pl_orbeccen',
       'pl_ratdor',
       'st_dens',
       'pl_orbsmax',
       'pl_orbper'
 ]
df = data[features].copy()

df_cleaned = df.dropna()

output_file_path = "./nasa_cleaned.csv"
df_cleaned.to_csv(output_file_path, index=False)

print(df_cleaned.shape[0])

data_clean = df_cleaned.dropna(subset=['pl_orbeccen'])
X = data_clean.drop(columns=['pl_orbeccen'])
y = data_clean['pl_orbeccen']

print(X.describe())
print("---------")
print(y.describe())

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)

from sklearn.linear_model import LinearRegression

linear_model = LinearRegression()
linear_model.fit(X_train, y_train)
y_pred_linear = linear_model.predict(X_test)
print(y_pred_linear.min())
print(y_pred_linear.max())
print(y_pred_linear.mean())

from sklearn.ensemble import RandomForestRegressor

rf_model = RandomForestRegressor()
rf_model.fit(X_train, y_train)
y_pred_rf = rf_model.predict(X_test)
print(y_pred_rf.min())
print(y_pred_rf.max())
print(y_pred_rf.mean())

from sklearn.svm import SVR

svr_model = SVR()
svr_model.fit(X_train, y_train)
y_pred_svr = svr_model.predict(X_test)
print(y_pred_svr.min())
print(y_pred_svr.max())
print(y_pred_svr.mean())

from sklearn.linear_model import Ridge

ridge_model = Ridge()
ridge_model.fit(X_train, y_train)
y_pred_ridge = ridge_model.predict(X_test)
print(y_pred_ridge.min())
print(y_pred_ridge.max())
print(y_pred_ridge.mean())

from sklearn.neighbors import KNeighborsRegressor

knn_model = KNeighborsRegressor()
knn_model.fit(X_train, y_train)
y_pred_knn = knn_model.predict(X_test)
print(y_pred_knn.min())
print(y_pred_knn.max())
print(y_pred_knn.mean())

from sklearn.ensemble import GradientBoostingRegressor

gb_model = GradientBoostingRegressor()
gb_model.fit(X_train, y_train)
y_pred_gb = gb_model.predict(X_test)
print(y_pred_gb.min())
print(y_pred_gb.max())
print(y_pred_gb.mean())

import xgboost as xgb

xgb_model = xgb.XGBRegressor()
xgb_model.fit(X_train, y_train)
y_pred_xgb = xgb_model.predict(X_test)
print(y_pred_xgb.min())
print(y_pred_xgb.max())
print(y_pred_xgb.mean())

from sklearn import linear_model
ls_model = linear_model.Lasso()
ls_model.fit(X_train, y_train)
y_pred_ls = ls_model.predict(X_test)
print(y_pred_ls.min())
print(y_pred_ls.max())
print(y_pred_ls.mean())

from sklearn.linear_model import Ridge
import numpy as np
n_samples, n_features = 10, 5
rng = np.random.RandomState(0)
y = rng.randn(n_samples)
X = rng.randn(n_samples, n_features)
rg_model = Ridge(alpha=1.0)
rg_model.fit(X, y)
X_test = rng.randn(n_samples, n_features)
y_pred_rg = rg_model.predict(X_test)
print(y_pred_rg.min())
print(y_pred_rg.max())
print(y_pred_rg.mean())

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, r2_score

def plot_model_performance(y_test, y_pred, model_name):
    y_test = np.array(y_test)
    y_pred = np.array(y_pred)

    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    sorted_indices = np.argsort(y_test)  #
    sorted_y_test = y_test[sorted_indices]
    sorted_y_pred = y_pred[sorted_indices]

    plt.figure(figsize=(10, 5))
    plt.scatter(range(len(sorted_y_test)), sorted_y_test, label='Actual Similarity', marker='o', color='blue')
    plt.scatter(range(len(sorted_y_pred)), sorted_y_pred, label='Predicted Similarity', marker='x', color='orange')
    plt.xlabel('Sorted Sample Index')
    plt.ylabel('Similarity')
    plt.title(f'Actual vs Predicted Similarity using {model_name}\nMSE: {mse:.4f}, RÂ²: {r2:.4f}')
    plt.legend()
    plt.grid()
    plt.show()

plot_model_performance(y_test, y_pred_linear, 'Linear Regression')
plot_model_performance(y_test, y_pred_rf, 'Random Forest')
plot_model_performance(y_test, y_pred_svr, 'Support Vector Machine')
plot_model_performance(y_test, y_pred_ridge, 'Ridge')
plot_model_performance(y_test, y_pred_knn, 'KNN')
plot_model_performance(y_test, y_pred_gb, 'Gradient Boosting')
plot_model_performance(y_test, y_pred_xgb, 'X Gradient Boosting')
plot_model_performance(y_test, y_pred_ls, 'Lasso')